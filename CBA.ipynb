{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "0d0bd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from frozendict import frozendict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "af4e6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "class Rule:\n",
    "    \n",
    "    def __init__(self, itemset, class_count, len_D):\n",
    "        self.itemset = itemset\n",
    "        self.class_count = class_count\n",
    "        \n",
    "        if len(set(class_count.values()))==1:\n",
    "            self.result = random.choice(class_count.keys())\n",
    "        else:\n",
    "            self.result = max(class_count, key=lambda x: class_count[x])\n",
    "        \n",
    "        self.conf = float(class_count[self.result])/sum(class_count.values())\n",
    "        self.sup = class_count[self.result] / float(len_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "112ee16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "class Rules:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules = {}\n",
    "        \n",
    "    \n",
    "    def get_rule(self, key):\n",
    "        length = len(key)\n",
    "        \n",
    "        if length not in self.rules:\n",
    "            return None\n",
    "        else:\n",
    "            return self.rules[length].get(key,None)\n",
    "        \n",
    "    def get_rules_by_length(self, length):\n",
    "        if length not in self.rules:\n",
    "            return []\n",
    "        else:\n",
    "            return self.rules[length].values()\n",
    "    \n",
    "    def get_itemset_by_length(self, length):\n",
    "        if length not in self.rules:\n",
    "            return set()\n",
    "        else:\n",
    "            return set(self.rules[length].keys())\n",
    "    \n",
    "    def get_itemset_rules_by_length(self, length):\n",
    "        if length not in self.rules:\n",
    "            return {}\n",
    "        else:\n",
    "            return self.rules[length]\n",
    "    \n",
    "    def add(self, rule):\n",
    "        if rule==None:\n",
    "            return\n",
    "        \n",
    "        length = len(rule.itemset)\n",
    "        \n",
    "        if length not in self.rules:\n",
    "            self.rules[length] = rule\n",
    "        else:\n",
    "            self.rules[length][rule.itemset] = rule\n",
    "    \n",
    "    def remove(self, itemset):\n",
    "        length = len(itemset)\n",
    "        \n",
    "        self.rules[length].pop(itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "daf06f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self, rule_builder):\n",
    "        self.rule_builder = rule_builder\n",
    "        self.rules = None\n",
    "        self.sorted_CARS = None\n",
    "        \n",
    "    def sort_rules(self, len_D):\n",
    "        sorted_CARS = []\n",
    "        for values in self.rule_builder.CARS.values():\n",
    "            sorted_CARS.extend(list(values.items()))\n",
    "        self.sorted_CARS = sorted(sorted_CARS, key=lambda x: (x[1][1][x[1][0]] / sum(x[1][1].values()), x[1][1][x[1][0]]/len_D, len(x[0])), reverse=True)\n",
    "    \n",
    "    def build_classifier(self, df, target_col):\n",
    "        len_D = len(df)\n",
    "        self.sort_rules(len_D)\n",
    "        temp_df = df\n",
    "        rules = []\n",
    "        for CARS in self.sorted_CARS:\n",
    "            cond, result = CARS\n",
    "            cond_df = temp_df.loc[(temp_df[list(cond)] == pd.Series(cond)).all(axis=1)]\n",
    "            correct = cond_df[cond_df[target_col]==result[0]]\n",
    "\n",
    "            if len(correct)!=0:\n",
    "                temp_df = temp_df.drop(index=cond_df.index) \n",
    "                if len(temp_df)==0:\n",
    "                    default_class = random.choice(df[target_col].unique())\n",
    "                else:\n",
    "                    default_class = temp_df[target_col].value_counts().idxmax()\n",
    "                total_error = (len(cond_df) - len(correct)) + len(temp_df[temp_df[target_col]!=default_class])\n",
    "                error = {'default': len(temp_df[temp_df[target_col]!=default_class]), 'class':(len(cond_df) - len(correct))}\n",
    "                rules.append([CARS, default_class, total_error, error])\n",
    "\n",
    "        lowest_error_id = np.argmin([x[2] for x in rules])\n",
    "        pruned_rules = rules[:lowest_error_id+1]\n",
    "        self.rules = pruned_rules\n",
    "        \n",
    "    def predict(self, df):\n",
    "        temp_df = df.copy()\n",
    "        ans = df.copy()\n",
    "        # Setting all to default class\n",
    "        ans['prediction'] = rules[-1][1]\n",
    "        for rule in self.rules:\n",
    "            cond, default_class, _, _ = rule\n",
    "            cond, prediction = cond\n",
    "            # Filtering rows that fulfil rule condition\n",
    "            cond_df = temp_df.loc[(temp_df[list(cond)] == pd.Series(cond)).all(axis=1)]\n",
    "            # Setting prediction\n",
    "            ans.loc[cond_df.index, 'prediction'] = prediction[0]\n",
    "            # Removing rows that has been predicted\n",
    "            temp_df = temp_df.drop(index=cond_df.index)\n",
    "\n",
    "        return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6be3af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleGenerator:\n",
    "    def __init__(self, min_sup=0.2, min_conf=0.6, weighted=False):\n",
    "        self.min_sup = min_sup\n",
    "        self.min_conf = min_conf\n",
    "        self.CARS = None\n",
    "        self.weighted = weighted\n",
    "    \n",
    "    def generate_rules(self, df, target_col):\n",
    "        len_D = len(df)\n",
    "        class_labels = df[target_col].unique()\n",
    "        variables = list(df.columns)\n",
    "        variables.remove(target_col)\n",
    "        k=1\n",
    "        F_1 = {}\n",
    "        for variable in variables:\n",
    "            temp = df.groupby(by=[variable, target_col]).size().unstack(level=1).reset_index()\n",
    "            temp = temp.fillna(0)\n",
    "        #     print(temp)\n",
    "            temp.apply(lambda x: F_1.update(self.convert_to_rule_items(x, [variable], class_labels, len_D)), axis=1)\n",
    "\n",
    "        self.CARS = {}\n",
    "        self.CARS[1] = self.gen_rules(F_1, len_D)\n",
    "\n",
    "        F = {}\n",
    "        F[1] = F_1\n",
    "        # print(F_1.keys())\n",
    "        frequent_variables = set([x for x in F_1.keys()])\n",
    "        # print(frequent_variables)\n",
    "        while len(F[k])!=0:\n",
    "            k+=1\n",
    "            # Includes generating candidate itemset and finding frequent itemset\n",
    "            F[k] = self.gen_itemset(F[k-1], df, target_col, frequent_variables)\n",
    "            frequent_variables = set([x for x in F[k].keys()])\n",
    "\n",
    "            # Build rules from frequent itemset\n",
    "            self.CARS[k] = self.gen_rules(F[k], len_D)\n",
    "            # Pruning rules\n",
    "            self.CARS[k] = self.prune_rules(self.CARS[k], self.CARS[k-1])\n",
    "        \n",
    "    def convert_to_rule_items(self, x, variables, class_label, len_D):\n",
    "        condset = {}\n",
    "        for variable in variables:\n",
    "            condset[variable] = x[variable]\n",
    "\n",
    "        condset = frozendict(condset)\n",
    "        class_count = {}\n",
    "        # When label don't exist in group\n",
    "        for label in class_label:\n",
    "            if label not in x:\n",
    "                class_count[label] = 0\n",
    "            else:\n",
    "                class_count[label] = x[label]\n",
    "        major_class = max(class_count, key=lambda x: class_count[x])\n",
    "\n",
    "        # Removing non frequent itemset\n",
    "#         if (class_count[major_class]/float(len_D))<self.min_sup:\n",
    "#             return {}\n",
    "\n",
    "        if self.weighted:\n",
    "            for label in class_label:\n",
    "                if (class_count[label] / float(len_D)) >= self.min_sup[label]:\n",
    "                    return {condset: (major_class, class_count)}\n",
    "        else:\n",
    "            if (class_count[major_class]/float(len_D))>=self.min_sup:\n",
    "                return {condset: (major_class, class_count)}\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def gen_rules(self, F_k, len_D):\n",
    "\n",
    "        rules = {}\n",
    "\n",
    "        for condset, (major_class, class_count) in F_k.items():\n",
    "\n",
    "            conf = class_count[major_class] / float(sum(class_count.values()))\n",
    "\n",
    "            if conf>self.min_conf:\n",
    "                # Checking if the support are the same for all classes\n",
    "                if set(class_count.values())==1:\n",
    "                    # Choose a random class\n",
    "                    major_class = random.choice(class_count.keys())\n",
    "\n",
    "                rules[condset] = (major_class, class_count)\n",
    "\n",
    "        return rules\n",
    "    \n",
    "    \n",
    "    def gen_itemset(self, F, df, target_col, itemsets):\n",
    "        F_new = {}\n",
    "        itemsets = set(itemsets)\n",
    "\n",
    "        condset = list(F.keys())\n",
    "\n",
    "        for i in range(len(condset)):\n",
    "            cond = condset[i]\n",
    "            temp = df.copy()\n",
    "            cols = []\n",
    "            for item in cond.items():\n",
    "                value = item[1]\n",
    "                col = item[0]\n",
    "                temp = temp[temp[col]==value]\n",
    "                cols.append(col)\n",
    "\n",
    "            for j in range(i+1, len(condset)):\n",
    "                itemset = condset[j]\n",
    "                #Line 20 to 25 is the Apriori principle, where we merge 2 frequent superset into a candidate key\n",
    "                # Checking if 2 itemsets differ only by 2 conditions\n",
    "                itemset_keys = itemset.keys()\n",
    "                cond_keys = cond.keys()\n",
    "                if len(set(itemset_keys) - set(cond_keys))==1 and len(set(itemset.items())^set(cond.items()))==2:\n",
    "                    variable = (set(itemset_keys) - set(cond_keys)).pop()\n",
    "                    value = itemset.get(variable)\n",
    "\n",
    "                    # Candidate generation\n",
    "                    temp_2 = temp[temp[variable]==value]\n",
    "                    groupby = cols+[variable, target_col]\n",
    "\n",
    "                    # Calculating frequency\n",
    "                    # TODO: Can be optimize further as we already filtered out the candidate, hence only need groupby class but this will affect convert_to_rule_items\n",
    "                    group = temp_2.groupby(by=groupby).size().unstack(level=-1).reset_index()\n",
    "                    group = group.fillna(0)\n",
    "                    # Converting candidates in frequent itemset\n",
    "                    group.apply(lambda x: F_new.update(convert_to_rule_items(x, groupby[:-1], class_labels, min_sup, len_D)), axis=1)\n",
    "\n",
    "\n",
    "        return F_new\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prune_rules(self, r, r_):\n",
    "        # r and r_ is a python dict\n",
    "        to_remove = []\n",
    "        for superset in r.keys():\n",
    "            # superset is a frozendict\n",
    "            superset_set = set(superset.items())\n",
    "            superset_value = r[superset]\n",
    "\n",
    "            for subset in r_.keys():\n",
    "                # subset is a frozendict\n",
    "                subset_set = set(subset.items())\n",
    "\n",
    "                if subset_set<superset_set:\n",
    "                    subset_value = r_[subset]\n",
    "\n",
    "                    superset_class = superset_value[0]\n",
    "                    superset_error = superset_value[1][superset_class] / sum(superset_value[1].values())\n",
    "\n",
    "                    subset_class = subset_value[0]\n",
    "                    subset_error = subset_value[1][subset_class] / sum(subset_value[1].values())\n",
    "\n",
    "                    if superset_error>=subset_error:\n",
    "                        to_remove.append(superset)\n",
    "                        # break to as the superset has already been removed, no further testing is needed\n",
    "                        break\n",
    "\n",
    "        for key in to_remove:\n",
    "            r.pop(key)\n",
    "        return r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
